# SM80 ç®€åŒ– MoE é›†æˆ - å¾…åŠäº‹é¡¹

## çŠ¶æ€è¯´æ˜
- âœ… å·²å®Œæˆ
- â³ è¿›è¡Œä¸­  
- âŒ å¾…å¼€å§‹
- ğŸ”’ è¢«é˜»å¡

---

## é˜¶æ®µ 1: æ ¸å¿ƒé›†æˆ (P0 - å¿…é¡»å®Œæˆ)

### 1.1 C++ æ‰©å±•å®ç°

- [âœ…] åˆ›å»º `flashinfer_simple_moe_sm80_ops.cu`
  - [âœ…] å®šä¹‰ `FusedMoeRunner` ç±»
  - [âœ…] å®ç° `get_tactic_num()` (è¿”å› 1)
  - [âœ…] å®ç° `run_gemm_profile()` (ç©ºæ“ä½œ)
  - [âœ…] å®ç° `run_moe()` ä¸»å‡½æ•°
  - [âœ…] æ·»åŠ  FP16/BF16/INT8 ç±»å‹åˆ†å‘
  - [âœ…] æ³¨å†Œåˆ° `torch.classes.fused_moe_sm80`

### 1.2 Python API ä¿®æ”¹

- [âœ…] ä¿®æ”¹ `flashinfer/fused_moe.py`
  - [âœ…] æ›´æ–° `gen_fused_moe_sm80_module()` 
  - [âœ…] ç§»é™¤ TRT-LLM/CUTLASS ä¾èµ–
  - [âœ…] æ·»åŠ ç®€åŒ–å®ç°è·¯å¾„
  - [âœ…] ä¿æŒç°æœ‰ `cutlass_fused_moe()` API ä¸å˜

### 1.3 GEMM å®ç° (å…³é”®è·¯å¾„)

- [âŒ] **å®ç° cuBLAS GEMM åç«¯**
  - [âŒ] åˆ›å»º `csrc/fused_moe_simple/cublas_gemm_impl.cu`
  - [âŒ] å®ç° `simple_moe_gemm1()` ä½¿ç”¨ `cublasGemmEx`
  - [âŒ] å®ç° `simple_moe_gemm2()` ä½¿ç”¨ `cublasGemmEx`
  - [âŒ] å¤„ç†åˆ†ç»„ GEMM (æ¯ä¸ªä¸“å®¶ç‹¬ç«‹è°ƒç”¨)
  - [âŒ] æ·»åŠ é”™è¯¯å¤„ç†

**ä¼˜å…ˆçº§**: ğŸ”´ **æœ€é«˜** - é˜»å¡æ‰€æœ‰æµ‹è¯•

**å½“å‰é˜»å¡**: æ— æ³•è¿è¡Œä»»ä½•å®é™…æµ‹è¯•ï¼Œå› ä¸ºæ²¡æœ‰ GEMM å®ç°

**å»ºè®®å®ç°**:
```cpp
// csrc/fused_moe_simple/cublas_gemm_impl.cu
#include <cublas_v2.h>
#include <cuda_fp16.h>

extern "C" void simple_moe_gemm1(
    void* output, void const* input, void const* weights, void const* bias,
    int64_t const* expert_offsets, int64_t M, int64_t N, int64_t K,
    int num_experts, bool is_fp16, cudaStream_t stream) {
  
  cublasHandle_t handle;
  cublasCreate(&handle);
  cublasSetStream(handle, stream);
  
  // å¯¹æ¯ä¸ªä¸“å®¶æ‰§è¡Œ GEMM
  for (int e = 0; e < num_experts; ++e) {
    int64_t start = expert_offsets[e];
    int64_t end = expert_offsets[e + 1];
    int64_t m = end - start;
    
    if (m == 0) continue;
    
    // GEMM: C = A @ B^T
    // A: [m, K] - è¾“å…¥
    // B: [N, K] - æƒé‡ (éœ€è¦è½¬ç½®)
    // C: [m, N] - è¾“å‡º
    
    if (is_fp16) {
      half* C = static_cast<half*>(output) + start * N;
      half const* A = static_cast<half const*>(input) + start * K;
      half const* B = static_cast<half const*>(weights) + e * N * K;
      
      float alpha = 1.0f, beta = 0.0f;
      cublasGemmEx(
          handle, CUBLAS_OP_T, CUBLAS_OP_N,
          N, m, K,
          &alpha,
          B, CUDA_R_16F, K,
          A, CUDA_R_16F, K,
          &beta,
          C, CUDA_R_16F, N,
          CUBLAS_COMPUTE_32F,
          CUBLAS_GEMM_DEFAULT_TENSOR_OP
      );
    }
  }
  
  cublasDestroy(handle);
}

// ç±»ä¼¼å®ç° simple_moe_gemm2()
```

### 1.4 ç¼–è¯‘ç³»ç»Ÿ

- [â³] éªŒè¯ JIT ç¼–è¯‘
  - [âŒ] æµ‹è¯•ç¼–è¯‘æ—¶é—´ (ç›®æ ‡ < 1 åˆ†é’Ÿ)
  - [âŒ] æµ‹è¯•ç¼“å­˜æœºåˆ¶
  - [âŒ] ç¡®ä¿åœ¨ä¸åŒ CUDA ç‰ˆæœ¬ä¸‹ç¼–è¯‘

**é¢„æœŸé—®é¢˜**:
1. Thrust lambda å¯èƒ½åœ¨æŸäº› CUDA ç‰ˆæœ¬ç¼–è¯‘å¤±è´¥
2. éœ€è¦é“¾æ¥ cuBLAS åº“ (`-lcublas`)

---

##é˜¶æ®µ 2: æµ‹è¯•ä¸éªŒè¯ (P0)

### 2.1 å•å…ƒæµ‹è¯•

- [âŒ] **åˆ›å»ºåŸºç¡€æµ‹è¯•**
  - [âŒ] æµ‹è¯• FP16 forward pass
  - [âŒ] æµ‹è¯• BF16 forward pass
  - [âŒ] æµ‹è¯• INT8 forward pass (é‡åŒ–)
  - [âŒ] æµ‹è¯•ä¸åŒ batch size (1, 16, 128, 1024)
  - [âŒ] æµ‹è¯•ä¸åŒä¸“å®¶æ•°é‡ (4, 8, 16, 64)
  - [âŒ] æµ‹è¯•ä¸åŒ top-k (1, 2, 4, 8)

**æµ‹è¯•è„šæœ¬æ¨¡æ¿**:
```python
# tests/test_sm80_simple_moe.py
import torch
import pytest
import flashinfer

@pytest.mark.parametrize("dtype", [torch.float16, torch.bfloat16])
@pytest.mark.parametrize("num_tokens", [16, 128, 1024])
@pytest.mark.parametrize("num_experts", [4, 8, 16])
@pytest.mark.parametrize("top_k", [1, 2, 4])
def test_sm80_moe_basic(dtype, num_tokens, num_experts, top_k):
    if not torch.cuda.is_available():
        pytest.skip("CUDA not available")
    
    device = torch.device("cuda:0")
    hidden_size = 512
    intermediate_size = 2048
    
    # åˆ›å»ºè¾“å…¥
    input_tensor = torch.randn(num_tokens, hidden_size, dtype=dtype, device=device)
    token_selected_experts = torch.randint(0, num_experts, (num_tokens, top_k), 
                                           dtype=torch.int32, device=device)
    token_final_scales = torch.softmax(torch.randn(num_tokens, top_k, device=device), dim=-1)
    
    # åˆ›å»ºæƒé‡
    fc1_weights = torch.randn(num_experts, 2 * intermediate_size, hidden_size,
                              dtype=dtype, device=device)
    fc2_weights = torch.randn(num_experts, hidden_size, intermediate_size,
                              dtype=dtype, device=device)
    
    # æ‰§è¡Œ
    output = flashinfer.cutlass_fused_moe(
        input=input_tensor,
        token_selected_experts=token_selected_experts,
        token_final_scales=token_final_scales,
        fc1_expert_weights=fc1_weights,
        fc2_expert_weights=fc2_weights,
        output_dtype=dtype,
        quant_scales=[],
    )
    
    # éªŒè¯
    assert output.shape == (num_tokens, hidden_size)
    assert output.dtype == dtype
    assert not torch.isnan(output).any()
    assert not torch.isinf(output).any()
```

### 2.2 æ•°å€¼æ­£ç¡®æ€§éªŒè¯

- [âŒ] **ä¸å‚è€ƒå®ç°å¯¹æ¯”**
  - [âŒ] å®ç° PyTorch åŸç”Ÿ MoE å‚è€ƒå®ç°
  - [âŒ] å¯¹æ¯”è¾“å‡ºå·®å¼‚ (ç›¸å¯¹è¯¯å·® < 1e-3)
  - [âŒ] æµ‹è¯•è¾¹ç•Œæƒ…å†µ (0 tokens, å•ä¸ª expert, ç­‰ç­‰)

**å‚è€ƒå®ç°**:
```python
def pytorch_reference_moe(input, token_selected_experts, token_final_scales,
                         fc1_weights, fc2_weights):
    """çº¯ PyTorch å®ç°ç”¨äºéªŒè¯æ­£ç¡®æ€§"""
    num_tokens, hidden_size = input.shape
    num_experts, inter_size, _ = fc1_weights.shape
    top_k = token_selected_experts.shape[1]
    
    output = torch.zeros(num_tokens, hidden_size, dtype=input.dtype, device=input.device)
    
    for token_idx in range(num_tokens):
        for k in range(top_k):
            expert_id = token_selected_experts[token_idx, k].item()
            weight = token_final_scales[token_idx, k].item()
            
            # GEMM1 + Activation
            hidden = torch.matmul(input[token_idx:token_idx+1], fc1_weights[expert_id].t())
            gate, up = hidden.split(inter_size // 2, dim=-1)
            activated = torch.nn.functional.silu(gate) * up
            
            # GEMM2
            expert_out = torch.matmul(activated, fc2_weights[expert_id].t())
            
            # ç´¯åŠ 
            output[token_idx] += weight * expert_out.squeeze(0)
    
    return output
```

### 2.3 é”™è¯¯å¤„ç†æµ‹è¯•

- [âŒ] æµ‹è¯•æ— æ•ˆè¾“å…¥
  - [âŒ] ç©ºè¾“å…¥å¼ é‡
  - [âŒ] ä¸åŒ¹é…çš„å½¢çŠ¶
  - [âŒ] é”™è¯¯çš„æ•°æ®ç±»å‹
  - [âŒ] æ— æ•ˆçš„ä¸“å®¶ç´¢å¼•
  - [âŒ] top_k > num_experts

---

## é˜¶æ®µ 3: æ€§èƒ½ä¼˜åŒ– (P1)

### 3.1 GEMM ä¼˜åŒ–

- [âŒ] **å®ç°è‡ªå®šä¹‰ Tensor Core GEMM**
  - [âŒ] åŸºç¡€ Tensor Core GEMM (FP16)
  - [âŒ] ä¼˜åŒ–å…±äº«å†…å­˜ä½¿ç”¨
  - [âŒ] å®ç°æµæ°´çº¿ (è½¯ä»¶æµæ°´çº¿)
  - [âŒ] æ·»åŠ  Async Copy (SM80 ç‰¹æ€§)
  - [âŒ] è°ƒä¼˜å—å¤§å°å’Œçº¿ç¨‹å¸ƒå±€

**æ€§èƒ½ç›®æ ‡**:
- cuBLAS: è¾¾åˆ° 70-90% çš„ TRT-LLM æ€§èƒ½
- è‡ªå®šä¹‰å†…æ ¸: è¾¾åˆ° 90-100% çš„ TRT-LLM æ€§èƒ½

### 3.2 å†…å­˜ä¼˜åŒ–

- [âŒ] å·¥ä½œç©ºé—´é‡ç”¨
  - [âŒ] å®ç°å·¥ä½œç©ºé—´æ± 
  - [âŒ] é¿å…é‡å¤åˆ†é…
  - [âŒ] ä¼˜åŒ–å³°å€¼å†…å­˜ä½¿ç”¨

### 3.3 Kernel Fusion

- [âŒ] èåˆæ¿€æ´»å‡½æ•°
  - [âŒ] Swiglu èåˆåˆ° GEMM1
  - [âŒ] å‡å°‘ä¸­é—´ç¼“å†²åŒº

### 3.4 æ€§èƒ½åŸºå‡†æµ‹è¯•

- [âŒ] **åˆ›å»ºåŸºå‡†æµ‹è¯•è„šæœ¬**
  - [âŒ] æµ‹è¯•ä¸åŒé…ç½® (batch, experts, hidden_size)
  - [âŒ] å¯¹æ¯” PyTorch åŸç”Ÿå®ç°
  - [âŒ] å¯¹æ¯” TRT-LLM (å¦‚æœå¯ç”¨)
  - [âŒ] ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š

**åŸºå‡†è„šæœ¬æ¨¡æ¿**:
```python
# benchmarks/bench_sm80_simple_moe.py
import torch
import time
import flashinfer

def benchmark_config(num_tokens, hidden_size, intermediate_size, 
                     num_experts, top_k, dtype, num_warmup=10, num_iters=100):
    device = torch.device("cuda:0")
    
    # åˆ›å»ºè¾“å…¥
    input_tensor = torch.randn(num_tokens, hidden_size, dtype=dtype, device=device)
    # ... (å…¶ä»–è¾“å…¥)
    
    # é¢„çƒ­
    for _ in range(num_warmup):
        output = flashinfer.cutlass_fused_moe(...)
    
    torch.cuda.synchronize()
    
    # è®¡æ—¶
    start = time.perf_counter()
    for _ in range(num_iters):
        output = flashinfer.cutlass_fused_moe(...)
    torch.cuda.synchronize()
    end = time.perf_counter()
    
    avg_time = (end - start) / num_iters
    return avg_time

# è¿è¡Œå¤šç§é…ç½®å¹¶ç”ŸæˆæŠ¥å‘Š
```

---

## é˜¶æ®µ 4: INT8 é‡åŒ–æ”¯æŒ (P1)

### 4.1 INT8 GEMM å®ç°

- [âŒ] å®ç° INT8 Tensor Core GEMM
  - [âŒ] GEMM1: INT8 è¾“å…¥ Ã— INT8 æƒé‡
  - [âŒ] GEMM2: INT8 è¾“å…¥ Ã— INT8 æƒé‡
  - [âŒ] æ·»åŠ åé‡åŒ–æ­¥éª¤

### 4.2 é‡åŒ–å·¥å…·

- [âŒ] æƒé‡é‡åŒ–å·¥å…·
  - [âŒ] Per-tensor é‡åŒ–
  - [âŒ] Per-channel é‡åŒ–
  - [âŒ] æ ¡å‡†æ•°æ®é›†æ”¯æŒ

### 4.3 INT8 æµ‹è¯•

- [âŒ] åˆ›å»º INT8 æµ‹è¯•å¥—ä»¶
- [âŒ] éªŒè¯ç²¾åº¦æŸå¤± (< 1% ç›¸å¯¹è¯¯å·®)

---

## é˜¶æ®µ 5: æ–‡æ¡£ä¸ç¤ºä¾‹ (P1)

### 5.1 æ–‡æ¡£å®Œå–„

- [âœ…] é›†æˆæ–¹æ¡ˆæ–‡æ¡£ (`SM80_SIMPLIFIED_MOE_INTEGRATION.md`)
- [â³] API æ–‡æ¡£
  - [âŒ] æ·»åŠ åˆ° `docs/` ç›®å½•
  - [âŒ] ç”Ÿæˆ API å‚è€ƒ
- [âŒ] æ€§èƒ½è°ƒä¼˜æŒ‡å—
- [âŒ] æ•…éšœæ’æŸ¥æŒ‡å—

### 5.2 ç¤ºä¾‹ä»£ç 

- [âœ…] åŸºç¡€ç¤ºä¾‹ (`sm80_moe_with_cutlass_fused_moe_api.py`)
- [âŒ] ç«¯åˆ°ç«¯ç¤ºä¾‹
  - [âŒ] å®Œæ•´çš„ Transformer å±‚
  - [âŒ] ä¸ Hugging Face é›†æˆ
- [âŒ] æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹

---

## é˜¶æ®µ 6: é«˜çº§ç‰¹æ€§ (P2 - å¯é€‰)

### 6.1 å¼ é‡å¹¶è¡Œæ”¯æŒ

- [âŒ] å®ç° TP (Tensor Parallelism)
  - [âŒ] NCCL é›†æˆ
  - [âŒ] All-Reduce æ“ä½œ
  - [âŒ] è·¨ GPU æµ‹è¯•

### 6.2 ä¸“å®¶å¹¶è¡Œæ”¯æŒ

- [âŒ] å®ç° EP (Expert Parallelism)
  - [âŒ] ä¸“å®¶åˆ†ç‰‡
  - [âŒ] All-to-All é€šä¿¡
  - [âŒ] è´Ÿè½½å‡è¡¡

### 6.3 æ›´å¤šæ¿€æ´»å‡½æ•°

- [âŒ] æ”¯æŒæ›´å¤šæ¿€æ´»
  - [âŒ] Gelu
  - [âŒ] Relu
  - [âŒ] Geglu

### 6.4 Layer Fusion

- [âŒ] èåˆ RMSNorm
- [âŒ] èåˆ LayerNorm
- [âŒ] èåˆ Dropout

---

## å·²çŸ¥é—®é¢˜è·Ÿè¸ª

### ğŸ› Issue #1: Thrust Lambda ç¼–è¯‘é—®é¢˜
**çŠ¶æ€**: ğŸ”’ å¾…è§£å†³  
**ä¼˜å…ˆçº§**: P1  
**æè¿°**: `prepareRouting()` ä¸­çš„ Thrust lambda åœ¨æŸäº› CUDA ç‰ˆæœ¬æ— æ³•ç¼–è¯‘  
**è§£å†³æ–¹æ¡ˆ**: ç”¨æ˜¾å¼ CUDA å†…æ ¸æ›¿æ¢ Thrust æ’åº  
**æ–‡ä»¶**: `csrc/fused_moe_simple/moe_runner.cuh:prepareRouting()`

### ğŸ› Issue #2: INT8 é‡åŒ–ç²¾åº¦
**çŠ¶æ€**: âŒ æœªå¼€å§‹  
**ä¼˜å…ˆçº§**: P1  
**æè¿°**: INT8 é‡åŒ–è·¯å¾„æœªå®ç°å®Œæ•´çš„ç¼©æ”¾å’Œé›¶ç‚¹æ”¯æŒ  
**è§£å†³æ–¹æ¡ˆ**: å®ç° Per-channel INT8 GEMM  
**æ–‡ä»¶**: `csrc/fused_moe_simple/flashinfer_simple_moe_sm80_ops.cu`

### ğŸ› Issue #3: cuBLAS Handle é‡ç”¨
**çŠ¶æ€**: âŒ æœªå¼€å§‹  
**ä¼˜å…ˆçº§**: P2  
**æè¿°**: æ¯æ¬¡ GEMM è°ƒç”¨éƒ½åˆ›å»º/é”€æ¯ cuBLAS handleï¼Œæ€§èƒ½å¼€é”€å¤§  
**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨çº¿ç¨‹å±€éƒ¨ handle æ±   

---

## é‡Œç¨‹ç¢‘

### M1: æ ¸å¿ƒåŠŸèƒ½å¯ç”¨ (ç›®æ ‡: 1 å‘¨)
- [âœ…] C++ æ‰©å±•å®ç°
- [âœ…] Python API é›†æˆ
- [âŒ] cuBLAS GEMM å®ç° â¬…ï¸ **å½“å‰é˜»å¡**
- [âŒ] åŸºç¡€æµ‹è¯•é€šè¿‡

### M2: ç”Ÿäº§å°±ç»ª (ç›®æ ‡: 2 å‘¨)
- [âŒ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [âŒ] æ€§èƒ½è¾¾åˆ° cuBLAS æ°´å¹³ (70-90%)
- [âŒ] æ–‡æ¡£å®Œå–„

### M3: æ€§èƒ½ä¼˜åŒ– (ç›®æ ‡: 4 å‘¨)
- [âŒ] è‡ªå®šä¹‰ GEMM å†…æ ¸
- [âŒ] æ€§èƒ½è¾¾åˆ° TRT-LLM æ°´å¹³ (90-100%)
- [âŒ] INT8 é‡åŒ–æ”¯æŒ

---

## ä¼˜å…ˆçº§æ€»ç»“

### ğŸ”´ P0 (å¿…é¡» - é˜»å¡å‘å¸ƒ)
1. âŒ å®ç° cuBLAS GEMM åç«¯
2. âŒ åŸºç¡€å•å…ƒæµ‹è¯•é€šè¿‡
3. âŒ æ•°å€¼æ­£ç¡®æ€§éªŒè¯

### ğŸŸ¡ P1 (é‡è¦ - å½±å“å¯ç”¨æ€§)
4. âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•
5. âŒ INT8 é‡åŒ–æ”¯æŒ
6. âŒ è‡ªå®šä¹‰ GEMM å†…æ ¸
7. â³ API æ–‡æ¡£

### ğŸŸ¢ P2 (å¯é€‰ - é”¦ä¸Šæ·»èŠ±)
8. âŒ å¼ é‡å¹¶è¡Œ/ä¸“å®¶å¹¶è¡Œ
9. âŒ æ›´å¤šæ¿€æ´»å‡½æ•°
10. âŒ Layer Fusion

---

## è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ï¼ä¼˜å…ˆå¤„ç† P0 å’Œ P1 ä»»åŠ¡ã€‚

**å¦‚ä½•è´¡çŒ®**:
1. ä»å¾…åŠäº‹é¡¹é€‰æ‹©ä¸€ä¸ªä»»åŠ¡
2. åˆ›å»ºåˆ†æ”¯: `git checkout -b feature/task-name`
3. å®ç°å¹¶æµ‹è¯•
4. æäº¤ PR å¹¶é“¾æ¥ç›¸å…³ Issue

**è”ç³»æ–¹å¼**:
- GitHub Issues: [flashinfer/issues](https://github.com/flashinfer-ai/flashinfer/issues)
- Discord: FlashInfer Community

---

æœ€åæ›´æ–°: 2025-01-XX  
ç»´æŠ¤è€…: FlashInfer Team
