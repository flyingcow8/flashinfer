# SM80 (Ampere) Support for cutlass_fused_moe API - Implementation Summary

## æ¦‚è¿°

æœ¬æ¬¡å®ç°ä¸º FlashInfer çš„ `cutlass_fused_moe` API æ·»åŠ äº†å¯¹ **SM80 (Ampere)** æ¶æ„çš„æ”¯æŒï¼ŒåŒ…æ‹¬ NVIDIA A100ã€A10ã€A30 ç­‰ GPUã€‚

## å®ç°çš„æ–‡ä»¶

### 1. Python å±‚ (`flashinfer/fused_moe.py`)

#### æ–°å¢å‡½æ•°ï¼š

- **`gen_fused_moe_sm80_module()`**: ç”Ÿæˆ SM80 çš„ JIT ç¼–è¯‘è§„èŒƒ
  - åŒ…å« FP16/BF16/FP32 kernel æºæ–‡ä»¶
  - ä¸åŒ…å« FP8/FP4 ç›¸å…³æ–‡ä»¶ï¼ˆSM80 ä¸æ”¯æŒï¼‰
  - ä½¿ç”¨ `-gencode=arch=compute_80,code=sm_80` ç­‰ SM80 ç³»åˆ—æ ‡å¿—

- **`get_fused_moe_sm80_module()`**: ç¼“å­˜çš„æ¨¡å—åŠ è½½å‡½æ•°
  - åˆ›å»º `MoERunner` ç±»æ¥ç®¡ç† kernel è¿è¡Œ
  - åŒ…å« SM80 é™åˆ¶æ£€æŸ¥ï¼ˆä¸æ”¯æŒ FP8/FP4/min_latency_modeï¼‰
  - å®ç° `cutlass_fused_moe_sm80` è‡ªå®šä¹‰æ“ä½œ

#### ä¿®æ”¹çš„å‡½æ•°ï¼š

- **`cutlass_fused_moe()`**: æ·»åŠ æ¶æ„è‡ªåŠ¨æ£€æµ‹
  ```python
  major, minor = get_compute_capability(input.device)
  
  if major == 8:  # SM80-SM89
      # ä½¿ç”¨ SM80 backend
      return get_fused_moe_sm80_module().cutlass_fused_moe_sm80(...)
  elif major >= 9:  # SM90+
      # ä½¿ç”¨ SM100 backend (Hopper/Blackwell)
      return get_fused_moe_sm100_module().cutlass_fused_moe_sm100(...)
  ```

### 2. C++ ç»‘å®šå±‚ (`csrc/fused_moe/cutlass_backend/flashinfer_cutlass_fused_moe_sm80_ops.cu`)

æ–°åˆ›å»ºçš„æ–‡ä»¶å®ç°ï¼š

- **`FusedMoeRunner` ç±»**: 
  - æ”¯æŒ FP16/FP16, BF16/BF16
  - æ”¯æŒ INT8 (UINT8) é‡åŒ–æƒé‡ï¼ˆéœ€è¦ FP16/BF16 æ¿€æ´»ï¼‰
  - æ”¯æŒ fused gated activation (Swiglu/Geglu)
  
- **é™åˆ¶æ£€æŸ¥**:
  - æ‹’ç» FP8 æ¿€æ´»/æƒé‡
  - æ‹’ç» FP4/INT4 æƒé‡ï¼ˆç¼–è¯‘é”™è¯¯ï¼‰
  - æ‹’ç» FP8 block scaling
  - æ‹’ç» W4A8 group scaling
  - æ‹’ç» min_latency_mode

- **Torch åº“æ³¨å†Œ**: `TORCH_LIBRARY(fused_moe_sm80, m)`

### 3. æµ‹è¯•æ–‡ä»¶ (`tests/test_sm80_fused_moe.py`)

åŒ…å«ä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹ï¼š

1. **`test_sm80_detection()`**: æ£€æµ‹ SM80 æ¶æ„
2. **`test_sm80_fused_moe_fp16()`**: FP16 åŠŸèƒ½æµ‹è¯•
3. **`test_sm80_fused_moe_bf16()`**: BF16 åŠŸèƒ½æµ‹è¯•
4. **`test_sm80_unsupported_fp8()`**: éªŒè¯æ­£ç¡®æ‹’ç» FP8
5. **`test_sm80_unsupported_min_latency_mode()`**: éªŒè¯æ­£ç¡®æ‹’ç» min_latency_mode

## æ”¯æŒçš„åŠŸèƒ½çŸ©é˜µ

### âœ… SM80 æ”¯æŒçš„åŠŸèƒ½

| åŠŸèƒ½ | æ”¯æŒçŠ¶æ€ |
|------|---------|
| FP16 æ¿€æ´»å’Œæƒé‡ | âœ… |
| BF16 æ¿€æ´»å’Œæƒé‡ | âœ… |
| INT8 (UINT8) é‡åŒ–æƒé‡ (with FP16/BF16 activations) | âœ… |
| Fused gated activation (Swiglu/Geglu) | âœ… |
| å¼ é‡å¹¶è¡Œ (TP) | âœ… |
| ä¸“å®¶å¹¶è¡Œ (EP) | âœ… |
| Top-K expert é€‰æ‹© | âœ… |

### âŒ SM80 ä¸æ”¯æŒçš„åŠŸèƒ½

| åŠŸèƒ½ | æ”¯æŒçŠ¶æ€ | åŸå›  |
|------|---------|------|
| FP8 æ¿€æ´»/æƒé‡ | âŒ | éœ€è¦ SM89+ (Ada) æˆ– SM90+ (Hopper) |
| FP4/INT4 æƒé‡ | âŒ | éœ€è¦ SM90+ (Hopper)ï¼Œç¼–è¯‘æ—¶ç¼ºå°‘ç±»å‹å®šä¹‰ |
| FP8 block scaling | âŒ | éœ€è¦ SM90+ (Hopper) |
| W4A8 group scaling | âŒ | éœ€è¦ SM90+ (Hopper) |
| Min latency mode | âŒ | éœ€è¦ SM90+ (Hopper) |
| FP32 æ¿€æ´»å’Œæƒé‡ | âŒ | å·²ç§»é™¤ä»¥ç®€åŒ– SM80 æ”¯æŒ |

## æ¶æ„å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   cutlass_fused_moe()                   â”‚
â”‚              (è‡ªåŠ¨æ£€æµ‹ GPU æ¶æ„å¹¶è·¯ç”±)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SM80 Backend     â”‚          â”‚  SM90+ Backend     â”‚
â”‚  (Ampere)         â”‚          â”‚  (Hopper/Blackwell)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… FP16/BF16      â”‚          â”‚ âœ… FP16/BF16/FP32  â”‚
â”‚ âœ… INT8 (UINT8)   â”‚          â”‚ âœ… FP8/FP4/INT4    â”‚
â”‚ âŒ FP8/FP4/INT4   â”‚          â”‚ âœ… Block scaling   â”‚
â”‚ âŒ Min latency    â”‚          â”‚ âœ… Min latency     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ä½¿ç”¨ç¤ºä¾‹

```python
import torch
from flashinfer import cutlass_fused_moe

# åœ¨ SM80 GPU ä¸Šä¼šè‡ªåŠ¨ä½¿ç”¨ SM80 backend
device = torch.device("cuda:0")

# è¾“å…¥
input = torch.randn(seq_len, hidden_size, dtype=torch.float16, device=device)
token_selected_experts = torch.randint(0, num_experts, (seq_len, top_k), device=device)
token_final_scales = torch.rand(seq_len, top_k, device=device)

# ä¸“å®¶æƒé‡
fc1_weights = torch.randn(num_experts, inter_size, hidden_size, dtype=torch.float16, device=device)
fc2_weights = torch.randn(num_experts, hidden_size, inter_size, dtype=torch.float16, device=device)

# è°ƒç”¨ - è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„ backend
output = cutlass_fused_moe(
    input=input,
    token_selected_experts=token_selected_experts,
    token_final_scales=token_final_scales,
    fc1_expert_weights=fc1_weights,
    fc2_expert_weights=fc2_weights,
    output_dtype=torch.float16,
    quant_scales=[],
)
```

## ç¼–è¯‘å’Œæµ‹è¯•

### ç¼–è¯‘

```bash
# è®¾ç½®ç›®æ ‡æ¶æ„
export TORCH_CUDA_ARCH_LIST="8.0 8.6 8.7"

# å®‰è£…ï¼ˆå¯ç¼–è¾‘æ¨¡å¼ï¼‰
python -m pip install --no-build-isolation -e . -v
```

### æµ‹è¯•

```bash
# è¿è¡Œ SM80 æµ‹è¯•
pytest tests/test_sm80_fused_moe.py -v

# æˆ–ç›´æ¥è¿è¡Œ
python tests/test_sm80_fused_moe.py
```

## å…¼å®¹æ€§

- **CUDA**: 11.8+
- **PyTorch**: 2.0+
- **GPU**: NVIDIA Ampere æ¶æ„
  - A100 (SM80)
  - A10 (SM86)
  - A30 (SM80)
  - RTX 30 ç³»åˆ— (SM86)
  - RTX 40 ç³»åˆ—ä¹Ÿå¯ä»¥ä½¿ç”¨ SM80 backendï¼Œä½†æ¨èä½¿ç”¨ SM89 ä¼˜åŒ–

## æ€§èƒ½è€ƒè™‘

- SM80 backend ä½¿ç”¨ CUTLASS æ¨¡æ¿åº“å®ç°é«˜æ•ˆ GEMM
- æ”¯æŒ fused gated activation å‡å°‘å†…å­˜è®¿é—®
- å¯¹äº FP8/FP4/INT4 workloadï¼Œå»ºè®®å‡çº§åˆ° Hopper+ GPU
- INT8 (UINT8) é‡åŒ–æƒé‡å¯åœ¨ SM80 ä¸Šè·å¾—è‰¯å¥½æ€§èƒ½

## åç»­å·¥ä½œ

1. âœ… SM80 åŸºç¡€æ”¯æŒ (å½“å‰å®ç°)
2. ğŸ”„ æ€§èƒ½è°ƒä¼˜å’Œ benchmark
3. ğŸ“ æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹
4. ğŸ“Š ä¸ SM90+ æ€§èƒ½å¯¹æ¯”æ–‡æ¡£

## è´¡çŒ®è€…

- å®ç°æ—¥æœŸ: 2025-10-17
- åŸºäº FlashInfer æ¶æ„å’Œç°æœ‰ SM100 å®ç°
