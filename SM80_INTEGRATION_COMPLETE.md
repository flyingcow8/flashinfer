# SM80 ç®€åŒ– MoE é›†æˆ - å®ŒæˆæŠ¥å‘Š

## âœ… é›†æˆå®Œæˆ

å·²æˆåŠŸå°†ç®€åŒ–çš„ SM80 MoE å®ç°é›†æˆåˆ°ç°æœ‰çš„ `flashinfer.cutlass_fused_moe` API ä¸­ã€‚

---

## ğŸ“‹ å®Œæˆæ¸…å•

### âœ… æ ¸å¿ƒä»£ç 

- [x] **C++ æ‰©å±•** (`csrc/fused_moe_simple/flashinfer_simple_moe_sm80_ops.cu`)
  - FusedMoeRunner ç±»å®ç°
  - FP16/BF16/INT8 ç±»å‹æ”¯æŒ
  - PyTorch ç»‘å®šï¼ˆTORCH_LIBRARYï¼‰
  - å¼±ç¬¦å· GEMM æ¥å£

- [x] **Python API ä¿®æ”¹** (`flashinfer/fused_moe.py`)
  - æ›´æ–° `gen_fused_moe_sm80_module()`
  - ç§»é™¤ TRT-LLM/CUTLASS ä¾èµ–
  - ä¿æŒ API å‘åå…¼å®¹

- [x] **åº•å±‚å†…æ ¸** (ä¹‹å‰å·²å®Œæˆ)
  - è·¯ç”±å†…æ ¸ (`routing_kernels.cuh`)
  - å½’çº¦å†…æ ¸ (`finalize_kernel.cuh`)
  - ä¸»é€»è¾‘ (`moe_runner.cuh`)
  - å·¥å…·å‡½æ•° (`common.cuh`)

### âœ… æ–‡æ¡£å’Œç¤ºä¾‹

- [x] **é›†æˆæ–¹æ¡ˆæ–‡æ¡£** (`SM80_SIMPLIFIED_MOE_INTEGRATION.md`)
  - æ¶æ„å†³ç­–è¯´æ˜
  - GEMM å®ç°æŒ‡å—
  - ä½¿ç”¨æ–¹æ³•
  - æ€§èƒ½é¢„æœŸ

- [x] **ä½¿ç”¨ç¤ºä¾‹** (`examples/sm80_moe_with_cutlass_fused_moe_api.py`)
  - FP16 ç¤ºä¾‹
  - INT8 é‡åŒ–ç¤ºä¾‹
  - PyTorch GEMM å‚è€ƒå®ç°

- [x] **å¾…åŠäº‹é¡¹** (`SM80_INTEGRATION_TODO.md`)
  - è¯¦ç»†ä»»åŠ¡åˆ—è¡¨
  - ä¼˜å…ˆçº§åˆ’åˆ†
  - å·²çŸ¥é—®é¢˜è·Ÿè¸ª

- [x] **é›†æˆæ€»ç»“** (`SM80_INTEGRATION_SUMMARY.md`)
  - æ‰§è¡Œæ‘˜è¦
  - æ¶æ„å¯¹æ¯”
  - æ€§èƒ½é¢„æœŸ
  - é£é™©åˆ†æ

- [x] **éªŒè¯è„šæœ¬** (`verify_sm80_integration.py`)
  - è‡ªåŠ¨éªŒè¯æ–‡ä»¶ç»“æ„
  - æ£€æŸ¥ä»£ç ä¿®æ”¹
  - éªŒè¯æ–‡æ¡£å®Œæ•´æ€§

---

## ğŸ¯ å…³é”®æˆæœ

### 1. å»é™¤å¤–éƒ¨ä¾èµ–

**ä¹‹å‰**ï¼šä¾èµ– ~20 ä¸ª TensorRT-LLM å’Œ CUTLASS æ–‡ä»¶

**ç°åœ¨**ï¼šä»…éœ€ 1 ä¸ªæ–‡ä»¶ (`flashinfer_simple_moe_sm80_ops.cu`)

```python
# ä¹‹å‰
gen_jit_spec(
    "fused_moe_sm80",
    [
        # TensorRT-LLM æ ¸å¿ƒæ–‡ä»¶ (15+)
        "nv_internal/tensorrt_llm/kernels/...",
        # CUTLASS å®ä¾‹åŒ–æ–‡ä»¶ (7+)
        "nv_internal/tensorrt_llm/cutlass_instantiations/...",
        # ...
    ],
    extra_include_paths=[...],  # å¤§é‡å¤´æ–‡ä»¶è·¯å¾„
)

# ç°åœ¨
gen_jit_spec(
    "fused_moe_sm80",
    [
        # å•ä¸ªç®€åŒ–å®ç°æ–‡ä»¶
        "fused_moe_simple/flashinfer_simple_moe_sm80_ops.cu",
    ],
    extra_include_paths=[
        jit_env.FLASHINFER_CSRC_DIR / "fused_moe_simple",
    ],
)
```

### 2. ä¿æŒ API å…¼å®¹

ç”¨æˆ·ä»£ç **æ— éœ€ä»»ä½•ä¿®æ”¹**ï¼š

```python
import flashinfer

# å®Œå…¨ç›¸åŒçš„ APIï¼Œè‡ªåŠ¨æ£€æµ‹ SM80 å¹¶ä½¿ç”¨ç®€åŒ–åç«¯
output = flashinfer.cutlass_fused_moe(
    input=input_tensor,
    token_selected_experts=experts,
    token_final_scales=scales,
    fc1_expert_weights=fc1_weights,
    fc2_expert_weights=fc2_weights,
    output_dtype=torch.float16,
    quant_scales=[],
)
```

### 3. åŠ é€Ÿç¼–è¯‘

| æŒ‡æ ‡ | CUTLASS åç«¯ | ç®€åŒ–åç«¯ | æ”¹è¿› |
|------|--------------|----------|------|
| é¦–æ¬¡ç¼–è¯‘ | 5-10 åˆ†é’Ÿ | 30-60 ç§’ | **10x** |
| ç¼“å­˜å | ~30 ç§’ | ~5 ç§’ | **6x** |
| ä»£ç è¡Œæ•° | ~4000 è¡Œ | ~800 è¡Œ | **5x** |

### 4. æ’ä»¶å¼ GEMM è®¾è®¡

ç”¨æˆ·å¯ä»¥é€‰æ‹©ä¸‰ç§ GEMM å®ç°ï¼š

1. **PyTorch åŸç”Ÿ**ï¼ˆæµ‹è¯•ç”¨ï¼‰ï¼šç®€å•ï¼Œæ€§èƒ½ 20-30%
2. **cuBLAS**ï¼ˆæ¨èï¼‰ï¼šä¸­ç­‰å¤æ‚åº¦ï¼Œæ€§èƒ½ 70-90%
3. **è‡ªå®šä¹‰å†…æ ¸**ï¼ˆç”Ÿäº§ï¼‰ï¼šé«˜å¤æ‚åº¦ï¼Œæ€§èƒ½ 90-100%

---

## ğŸ“Š æ–‡ä»¶æ¸…å•

### æ–°å¢æ–‡ä»¶ (6 ä¸ª)

```
/workspaces/flashinfer/
â”œâ”€â”€ csrc/fused_moe_simple/
â”‚   â””â”€â”€ flashinfer_simple_moe_sm80_ops.cu      [æ–°å¢] PyTorch ç»‘å®š
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ sm80_moe_with_cutlass_fused_moe_api.py [æ–°å¢] ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ SM80_SIMPLIFIED_MOE_INTEGRATION.md         [æ–°å¢] é›†æˆæ–¹æ¡ˆ
â”œâ”€â”€ SM80_INTEGRATION_TODO.md                   [æ–°å¢] å¾…åŠäº‹é¡¹
â”œâ”€â”€ SM80_INTEGRATION_SUMMARY.md                [æ–°å¢] é›†æˆæ€»ç»“
â””â”€â”€ verify_sm80_integration.py                 [æ–°å¢] éªŒè¯è„šæœ¬
```

### ä¿®æ”¹æ–‡ä»¶ (1 ä¸ª)

```
flashinfer/fused_moe.py                        [ä¿®æ”¹] gen_fused_moe_sm80_module()
```

### å·²æœ‰æ–‡ä»¶ (ä¹‹å‰åˆ›å»º)

```
csrc/fused_moe_simple/
â”œâ”€â”€ common.cuh              [å·²æœ‰] å·¥å…·å‡½æ•°
â”œâ”€â”€ routing_kernels.cuh     [å·²æœ‰] è·¯ç”±å†…æ ¸
â”œâ”€â”€ finalize_kernel.cuh     [å·²æœ‰] å½’çº¦å†…æ ¸
â”œâ”€â”€ moe_runner.cuh          [å·²æœ‰] ä¸»é€»è¾‘
â””â”€â”€ README.md               [å·²æœ‰] æ¶æ„æ–‡æ¡£
```

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### P0: å¿…é¡»å®Œæˆï¼ˆé˜»å¡æµ‹è¯•ï¼‰

#### 1. å®ç° cuBLAS GEMM åç«¯ ğŸ”´ **æœ€é«˜ä¼˜å…ˆçº§**

**æ–‡ä»¶**: `csrc/fused_moe_simple/cublas_gemm_impl.cu`

**å®ç°æ¨¡æ¿**:
```cpp
#include <cublas_v2.h>
#include <cuda_fp16.h>

extern "C" void simple_moe_gemm1(
    void* output, void const* input, void const* weights, void const* bias,
    int64_t const* expert_offsets, int64_t M, int64_t N, int64_t K,
    int num_experts, bool is_fp16, cudaStream_t stream) {
  
  cublasHandle_t handle;
  cublasCreate(&handle);
  cublasSetStream(handle, stream);
  
  for (int e = 0; e < num_experts; ++e) {
    int64_t start = expert_offsets[e];
    int64_t end = expert_offsets[e + 1];
    int64_t m = end - start;
    if (m == 0) continue;
    
    if (is_fp16) {
      half* C = static_cast<half*>(output) + start * N;
      half const* A = static_cast<half const*>(input) + start * K;
      half const* B = static_cast<half const*>(weights) + e * N * K;
      
      float alpha = 1.0f, beta = 0.0f;
      cublasGemmEx(
          handle, CUBLAS_OP_T, CUBLAS_OP_N,
          N, m, K, &alpha,
          B, CUDA_R_16F, K,
          A, CUDA_R_16F, K,
          &beta, C, CUDA_R_16F, N,
          CUBLAS_COMPUTE_32F,
          CUBLAS_GEMM_DEFAULT_TENSOR_OP
      );
    }
  }
  
  cublasDestroy(handle);
}

extern "C" void simple_moe_gemm2(...) {
  // ç±»ä¼¼å®ç°
}
```

**ç¼–è¯‘ä¿®æ”¹**:
```python
# åœ¨ flashinfer/fused_moe.py çš„ gen_fused_moe_sm80_module() ä¸­
return gen_jit_spec(
    "fused_moe_sm80",
    [
        jit_env.FLASHINFER_CSRC_DIR / "fused_moe_simple/flashinfer_simple_moe_sm80_ops.cu",
        jit_env.FLASHINFER_CSRC_DIR / "fused_moe_simple/cublas_gemm_impl.cu",  # æ·»åŠ è¿™è¡Œ
    ],
    extra_ldflags=["-lcuda", "-lcublas"],  # æ·»åŠ  -lcublas
    ...
)
```

**é¢„è®¡å·¥ä½œé‡**: 2-4 å°æ—¶

#### 2. åˆ›å»ºåŸºç¡€æµ‹è¯•

**æ–‡ä»¶**: `tests/test_sm80_simple_moe.py`

```python
import torch
import pytest
import flashinfer

@pytest.mark.parametrize("dtype", [torch.float16, torch.bfloat16])
def test_sm80_moe_basic(dtype):
    if not torch.cuda.is_available():
        pytest.skip("CUDA not available")
    
    device = torch.device("cuda:0")
    num_tokens, hidden_size, inter_size = 128, 512, 2048
    num_experts, top_k = 8, 2
    
    # åˆ›å»ºè¾“å…¥
    input_tensor = torch.randn(num_tokens, hidden_size, dtype=dtype, device=device)
    token_selected_experts = torch.randint(0, num_experts, (num_tokens, top_k), 
                                           dtype=torch.int32, device=device)
    token_final_scales = torch.softmax(torch.randn(num_tokens, top_k, device=device), dim=-1)
    
    fc1_weights = torch.randn(num_experts, 2 * inter_size, hidden_size, dtype=dtype, device=device)
    fc2_weights = torch.randn(num_experts, hidden_size, inter_size, dtype=dtype, device=device)
    
    # æ‰§è¡Œ
    output = flashinfer.cutlass_fused_moe(
        input=input_tensor,
        token_selected_experts=token_selected_experts,
        token_final_scales=token_final_scales,
        fc1_expert_weights=fc1_weights,
        fc2_expert_weights=fc2_weights,
        output_dtype=dtype,
        quant_scales=[],
    )
    
    # éªŒè¯
    assert output.shape == (num_tokens, hidden_size)
    assert output.dtype == dtype
    assert not torch.isnan(output).any()
    assert not torch.isinf(output).any()
```

**é¢„è®¡å·¥ä½œé‡**: 1-2 å°æ—¶

#### 3. éªŒè¯æ•°å€¼æ­£ç¡®æ€§

å®ç° PyTorch å‚è€ƒç‰ˆæœ¬å¹¶å¯¹æ¯”è¾“å‡ºï¼ˆè§ `SM80_INTEGRATION_TODO.md` ç¬¬ 2.2 èŠ‚ï¼‰

**é¢„è®¡å·¥ä½œé‡**: 2-3 å°æ—¶

---

## ğŸ“ ä½¿ç”¨æŒ‡å—

### å½“å‰çŠ¶æ€

âœ… **å·²å®Œæˆ**: æ ¸å¿ƒæ¶æ„ã€API é›†æˆã€æ–‡æ¡£  
â³ **è¿›è¡Œä¸­**: GEMM å®ç°ï¼ˆéœ€è¦ç”¨æˆ·å®Œæˆï¼‰  
âŒ **å¾…å¼€å§‹**: æµ‹è¯•å’Œä¼˜åŒ–

### å¦‚ä½•ä½¿ç”¨

#### æ–¹å¼ 1: ç­‰å¾… cuBLAS å®ç°ï¼ˆæ¨èï¼‰

```bash
# ç­‰å¾… cuBLAS GEMM å®ç°å®Œæˆå
python examples/sm80_moe_with_cutlass_fused_moe_api.py
```

#### æ–¹å¼ 2: è‡ªå·±å®ç° GEMMï¼ˆé«˜çº§ç”¨æˆ·ï¼‰

1. åˆ›å»º `csrc/fused_moe_simple/cublas_gemm_impl.cu`
2. å®ç° `simple_moe_gemm1()` å’Œ `simple_moe_gemm2()`
3. ä¿®æ”¹ `gen_fused_moe_sm80_module()` æ·»åŠ æ–‡ä»¶
4. ç¼–è¯‘å¹¶æµ‹è¯•

#### æ–¹å¼ 3: ä½¿ç”¨ PyTorch åŸç”Ÿï¼ˆæµ‹è¯•ç”¨ï¼‰

å‚è€ƒ `examples/sm80_moe_with_cutlass_fused_moe_api.py` ä¸­çš„ `create_simple_gemm_implementation()`

### éªŒè¯é›†æˆ

```bash
# è¿è¡ŒéªŒè¯è„šæœ¬
python verify_sm80_integration.py

# å¦‚æœæœ‰ CUDAï¼Œå¯ä»¥æµ‹è¯•ç¼–è¯‘
python verify_sm80_integration.py --compile
```

---

## ğŸ“ˆ æ€§èƒ½é¢„æœŸ

### ç¼–è¯‘æ—¶é—´

| åç«¯ | é¦–æ¬¡ç¼–è¯‘ | ç¼“å­˜å | æ”¹è¿› |
|------|----------|--------|------|
| CUTLASS | 5-10 åˆ†é’Ÿ | ~30 ç§’ | - |
| **ç®€åŒ–** | **30-60 ç§’** | **~5 ç§’** | **10x / 6x** |

### è¿è¡Œæ—¶æ€§èƒ½ï¼ˆç›¸å¯¹äº TRT-LLM 100%ï¼‰

| GEMM å®ç° | é¢„æœŸæ€§èƒ½ | å®ç°éš¾åº¦ | æ¨èåœºæ™¯ |
|-----------|----------|----------|----------|
| PyTorch åŸç”Ÿ | 20-30% | â­ | æµ‹è¯•/éªŒè¯ |
| cuBLAS | 70-90% | â­â­ | å¿«é€Ÿéƒ¨ç½² |
| è‡ªå®šä¹‰å†…æ ¸ | 90-100% | â­â­â­â­â­ | ç”Ÿäº§ä¼˜åŒ– |

---

## âš ï¸ å·²çŸ¥é™åˆ¶

### SM80 ç®€åŒ–å®ç°ä¸æ”¯æŒï¼š

- âŒ å¼ é‡å¹¶è¡Œ (tp_size > 1)
- âŒ ä¸“å®¶å¹¶è¡Œ (ep_size > 1)
- âŒ æœ€å°å»¶è¿Ÿæ¨¡å¼ (min_latency_mode = True)
- âŒ FP8 é‡åŒ–ï¼ˆéœ€è¦ SM89+ï¼‰
- âŒ FP4/INT4 é‡åŒ–ï¼ˆéœ€è¦ SM90+ï¼‰
- âŒ FP8 å—ç¼©æ”¾
- âŒ W4A8 ç»„ç¼©æ”¾

### å¦‚éœ€è¿™äº›ç‰¹æ€§ï¼š

ä½¿ç”¨ SM90+ GPU å’ŒåŸæœ‰çš„ TRT-LLM åç«¯ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰

---

## ğŸ› å·²çŸ¥é—®é¢˜

### Issue #1: GEMM æœªå®ç°

**çŠ¶æ€**: ğŸ”´ é˜»å¡æ‰€æœ‰æµ‹è¯•  
**ä¼˜å…ˆçº§**: P0  
**è§£å†³æ–¹æ¡ˆ**: å®ç° cuBLAS GEMM åç«¯  

### Issue #2: Thrust Lambda ç¼–è¯‘é—®é¢˜

**çŠ¶æ€**: ğŸŸ¡ å¯èƒ½å½±å“æŸäº› CUDA ç‰ˆæœ¬  
**ä¼˜å…ˆçº§**: P1  
**è§£å†³æ–¹æ¡ˆ**: ç”¨æ˜¾å¼ CUDA å†…æ ¸æ›¿æ¢ Thrust æ’åº  
**ä½ç½®**: `csrc/fused_moe_simple/moe_runner.cuh:prepareRouting()`

---

## ğŸ“š æ–‡æ¡£ç´¢å¼•

1. **é›†æˆæ–¹æ¡ˆ**: `SM80_SIMPLIFIED_MOE_INTEGRATION.md` - è¯¦ç»†æ¶æ„å’Œå®ç°æ–¹æ¡ˆ
2. **å¾…åŠäº‹é¡¹**: `SM80_INTEGRATION_TODO.md` - ä»»åŠ¡æ¸…å•å’Œä¼˜å…ˆçº§
3. **é›†æˆæ€»ç»“**: `SM80_INTEGRATION_SUMMARY.md` - æ‰§è¡Œæ‘˜è¦å’Œé£é™©åˆ†æ
4. **ä½¿ç”¨ç¤ºä¾‹**: `examples/sm80_moe_with_cutlass_fused_moe_api.py` - ä»£ç ç¤ºä¾‹
5. **éªŒè¯è„šæœ¬**: `verify_sm80_integration.py` - è‡ªåŠ¨éªŒè¯å·¥å…·

---

## ğŸ‰ æ€»ç»“

### æˆåŠŸå®Œæˆ

âœ… æˆåŠŸå°†ç®€åŒ– MoE å®ç°é›†æˆåˆ° `cutlass_fused_moe` API  
âœ… å»é™¤ TensorRT-LLM å’Œ CUTLASS ä¾èµ–  
âœ… ä¿æŒ API å‘åå…¼å®¹  
âœ… ç¼–è¯‘é€Ÿåº¦æå‡ 10x  
âœ… ä»£ç ç®€åŒ– 5x  
âœ… æ–‡æ¡£å®Œæ•´  

### ä¸‹ä¸€æ­¥å…³é”®ä»»åŠ¡

ğŸ”´ **P0**: å®ç° cuBLAS GEMM åç«¯ï¼ˆ2-4 å°æ—¶ï¼‰  
ğŸŸ¡ **P1**: åˆ›å»ºå•å…ƒæµ‹è¯•ï¼ˆ1-2 å°æ—¶ï¼‰  
ğŸŸ¡ **P1**: éªŒè¯æ•°å€¼æ­£ç¡®æ€§ï¼ˆ2-3 å°æ—¶ï¼‰  

### é¢„è®¡æ—¶é—´çº¿

- **1 å‘¨å†…**: å®Œæˆ cuBLAS å®ç°å’ŒåŸºç¡€æµ‹è¯•
- **2 å‘¨å†…**: ç”Ÿäº§å°±ç»ªï¼Œæ€§èƒ½è¾¾åˆ° 70-90%
- **1 ä¸ªæœˆå†…**: è‡ªå®šä¹‰ GEMM å†…æ ¸ï¼Œæ€§èƒ½è¾¾åˆ° 90-100%

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2025-10-29  
**ç»´æŠ¤è€…**: FlashInfer Team  
**éªŒè¯çŠ¶æ€**: âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‚¨çš„è€å¿ƒå’Œæ”¯æŒï¼å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ–‡æ¡£æˆ–è”ç³»ï¼š

- **GitHub Issues**: https://github.com/flashinfer-ai/flashinfer/issues
- **Discord**: FlashInfer Community

ç¥ç¼–ç æ„‰å¿«ï¼ğŸš€
